{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary Classification with Sonar Dataset: Baseline\n",
    "from pandas import read_csv, read_pickle\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Concatenate, Input, Dense, Embedding, Flatten, Dropout, BatchNormalization, SpatialDropout1D\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import  Adam\n",
    "import tensorflow.keras.backend as k\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "import tqdm\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strLower(s):\n",
    "    return str(s).lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "train = read_pickle(\"input/train_prepared.pickle\")\n",
    "test = read_pickle(\"input/test_prepared.pickle\")\n",
    "sample_submission = read_csv('./input/sample_submission.csv', index_col='TransactionID')\n",
    "features = read_csv(\"input/feature_importances0.csv\", index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=train.sort_values(by=['TransactionID'])\n",
    "train=train.set_index('TransactionID')\n",
    "train=train.drop(['TransactionDT'],axis=1)\n",
    "train=train.drop(['Unnamed: 0'],axis=1)\n",
    "\n",
    "test=test.sort_values(by=['TransactionID'])\n",
    "test=test.set_index('TransactionID')\n",
    "test=test.drop(['TransactionDT'],axis=1)\n",
    "test=test.drop(['Unnamed: 0'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count the number of nulls per row\n",
    "train['numNaN']=train.isna().sum(axis=1)\n",
    "test['numNaN']=test.isna().sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change all strings in columns to lowercase\n",
    "for i in range(len(train.dtypes.values)):\n",
    "    if \"object\" in str(train.dtypes.values[i]):\n",
    "        col = train.columns[i]\n",
    "        train[col]=train[col].apply(strLower)\n",
    "        test[col]=test[col].apply(strLower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train=train.drop(['ProductCD', 'addr1', 'M2'],axis=1)\n",
    "#test=test.drop(['ProductCD', 'addr1', 'M2'],axis=1)\n",
    "\n",
    "#categorise each column into either continuous or discrete\n",
    "######browser + browser version tgt and OS and OS version\n",
    "continuous = ['TransactionAmt','dist1','numNaN'] + list(train.filter(regex='^C[0-9]')) + list(train.filter(regex='^D[0-9]')) + list(train.filter(regex='^V'))\n",
    "discrete = ['ProductCD','addr1','addr2','P_emaildomain','R_emaildomain',\n",
    "            'DeviceType', 'DeviceInfo', 'OS_id_30', 'version_id_30', 'browser',\n",
    " 'b_version',] + list(train.filter(regex='^card')) + list(train.filter(regex='^M')) + list(train.filter(regex='^id_'))\n",
    "\n",
    "top_50 = ['card1', 'card2', 'addr1', 'TransactionAmt', 'D15', 'dist1', 'D4', 'D2', 'D10', 'card5', 'D11', 'C13', 'D1', 'id_02', 'P_emaildomain', 'D5', 'id_20', 'D3', 'id_19', 'C1', 'D8', 'C2', 'b_version', 'V307', 'V310', 'C14', 'C6', 'C11', 'V313', 'C9', 'V127', 'id_13', 'V130', 'D9', 'id_06', 'V315', 'V314', 'M4', 'V308', 'R_emaildomain', 'DeviceInfo', 'id_05', 'M5', 'V312', 'C5', 'card4', 'id_33', 'id_01', 'M6', 'V317']\n",
    "\n",
    "dropped = ['TransactionAmt', 'dist1', 'C1', 'C5', 'C7', 'C9', 'C13', 'D1', 'D2', 'D3', 'D4', 'D5', 'D6', 'D8', 'D9', 'D10', 'D11', 'D12', 'D13', 'D14', 'D15', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V12', 'V13', 'V15', 'V17', 'V19', 'V20', 'V29', 'V31', 'V35', 'V36', 'V37', 'V38', 'V39', 'V40', 'V41', 'V42', 'V44', 'V45', 'V46', 'V47', 'V48', 'V50', 'V51', 'V53', 'V54', 'V56', 'V57', 'V59', 'V61', 'V62', 'V63', 'V64', 'V69', 'V71', 'V73', 'V75', 'V76', 'V78', 'V79', 'V80', 'V82', 'V83', 'V84', 'V85', 'V87', 'V90', 'V92', 'V95', 'V96', 'V99', 'V100', 'V130', 'V131', 'V138', 'V139', 'V140', 'V141', 'V142', 'V144', 'V146', 'V147', 'V148', 'V151', 'V152', 'V157', 'V161', 'V169', 'V170', 'V171', 'V172', 'V173', 'V174', 'V175', 'V176', 'V180', 'V181', 'V183', 'V184', 'V185', 'V186', 'V187', 'V188', 'V189', 'V190', 'V191', 'V194', 'V195', 'V197', 'V198', 'V200', 'V201', 'V205', 'V206', 'V208', 'V214', 'V217', 'V218', 'V220', 'V221', 'V223', 'V224', 'V226', 'V227', 'V228', 'V229', 'V230', 'V234', 'V235', 'V236', 'V238', 'V239', 'V240', 'V241', 'V242', 'V243', 'V245', 'V246', 'V247', 'V248', 'V250', 'V252', 'V255', 'V257', 'V258', 'V260', 'V261', 'V262', 'V263', 'V266', 'V267', 'V270', 'V276', 'V282', 'V283', 'V285', 'V287', 'V288', 'V289', 'V291', 'V294', 'V302', 'V303', 'V310', 'V312', 'V313', 'V314', 'V325', 'V326', 'V327', 'V328', 'V329', 'V334']\n",
    "lgbm_feature = list(features.feature)\n",
    "\n",
    "continuous = [x for x in continuous if x in top_50]\n",
    "discrete = [x for x in discrete if x in top_50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discrete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#Fill nan with zeroes for NN for continuos values\n",
    "#Fill nan with \"other\" for NN for discrete values\n",
    "def fill_nan(df):\n",
    "    for x in list(df.columns.values):\n",
    "        if x in continuous:\n",
    "            df[x] = df[x].fillna(0)\n",
    "            \n",
    "        elif x in discrete:\n",
    "            df[x] = df[x].replace(\"nan\", \"other\")\n",
    "            df[x] = df[x].replace(np.nan, \"other\")\n",
    "    return df\n",
    "\n",
    "train=fill_nan(train)\n",
    "test=fill_nan(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Label encoding\n",
    "label_counts = {}\n",
    "for col in tqdm.tqdm(train.columns):\n",
    "    if col in discrete:\n",
    "        le = LabelEncoder()\n",
    "        le.fit(list(train[col].values) + list(test[col].values))\n",
    "        train[col] = le.transform(list(train[col].values))\n",
    "        test[col] = le.transform(list(test[col].values))\n",
    "        label_counts[col]=len(list(le.classes_)) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in continuous:\n",
    "    scaler = StandardScaler()\n",
    "    if train[col].max() > 100 and train[col].min() >= 0:\n",
    "        train[col] = np.log1p(train[col])\n",
    "        test[col] = np.log1p(test[col])\n",
    "    scaler.fit(np.concatenate([train[col].values.reshape(-1,1), test[col].values.reshape(-1,1)]))\n",
    "    train[col] = scaler.transform(train[col].values.reshape(-1,1))\n",
    "    test[col] = scaler.transform(test[col].values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'isFraud'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split training set into training and validation set\n",
    "train_set, valid_set = train_test_split(train, test_size = 0.2, random_state = 4041, shuffle = False)\n",
    "folds = StratifiedKFolds(n_splits=5,shuffle=False,random_state=4041)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert each column in the discrete list into a tensor and append into a list\n",
    "def create_model():\n",
    "    k.clear_session()\n",
    "    categorical_inputs = []\n",
    "    for cat in discrete:\n",
    "        categorical_inputs.append(Input(shape=[1], name=cat))\n",
    "\n",
    "    #connect each embedding for each columns with their respective input\n",
    "    categorical_embeddings = []\n",
    "    for i, cat in enumerate(discrete):\n",
    "        categorical_embeddings.append(\n",
    "            Embedding(label_counts[cat], int(np.log1p(label_counts[cat]) + 1), name = cat + \"_embed\")(categorical_inputs[i]))\n",
    "\n",
    "    categorical_logits = Concatenate(name = \"categorical_conc\")([Flatten()(SpatialDropout1D(.1)(cat_emb)) for cat_emb in categorical_embeddings])\n",
    "\n",
    "    #Convert continuous columns into tensors\n",
    "    numerical_inputs = Input(shape=[train[continuous].shape[1]], name = 'continuous')\n",
    "    numerical_logits = Dropout(.1)(numerical_inputs)\n",
    "\n",
    "    #Join in the 2 logits together to form 1 layer\n",
    "    x = Concatenate()([categorical_logits, numerical_logits,])\n",
    "\n",
    "    #Build the hidden layer and output to 1 node\n",
    "    x = Dense(200, activation = 'relu')(x)\n",
    "    x = Dropout(.2)(x)\n",
    "    x = Dense(100, activation = 'relu')(x)\n",
    "    x = Dropout(.2)(x)\n",
    "    out = Dense(1, activation = 'sigmoid')(x)\n",
    "\n",
    "    model = Model(inputs=categorical_inputs + [numerical_inputs],outputs=out)\n",
    "    loss = \"binary_crossentropy\"\n",
    "    model.compile(optimizer=Adam(lr = 0.01), loss = loss)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discrete_backup = deepcopy(discrete)\n",
    "continuous_backup = deepcopy(continuous)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_features(df):\n",
    "    X = {'continuous':np.array(df[continuous])}\n",
    "    for cat in discrete:\n",
    "        X[cat] = np.array(df[cat])\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = get_input_features(test)\n",
    "for fold,(trn_idx,test_idx) in enumerate(folds.split(train,target)):\n",
    "    train_set = train.iloc[trn_idx]\n",
    "    valid_set = train.iloc[test_idx]\n",
    "    X_train = get_input_features(train_set)\n",
    "    X_valid = get_input_features(valid_set)\n",
    "    y_train = train_set[target]\n",
    "    y_valid = valid_set[target]\n",
    "\n",
    "    model = create_model()\n",
    "\n",
    "    best_score = 0\n",
    "    patience = 0\n",
    "\n",
    "    for i in range(100):\n",
    "        if patience < 3:\n",
    "            hist = model.fit(X_train, y_train, validation_data = (X_valid,y_valid), batch_size = 8000, epochs = 1, verbose = 1)\n",
    "            valid_preds = model.predict(X_valid, batch_size =  8000, verbose = True)\n",
    "            score = roc_auc_score(y_valid, valid_preds)\n",
    "            print(score)\n",
    "            if score > best_score:\n",
    "                model.save_weights(\"best_model.h5\")\n",
    "                best_score = score\n",
    "                patience = 0\n",
    "            else:\n",
    "                patience += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"best_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid = get_input_features(valid_set)\n",
    "X_test = get_input_features(test)\n",
    "valid_preds = model.predict(X_valid, batch_size = 500, verbose = True)\n",
    "score = roc_auc_score(y_valid, valid_preds)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = model.fit(X_valid,y_valid, batch_size = 8000, epochs = 3, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test, batch_size = 2000, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission[target] = predictions\n",
    "sample_submission.to_csv('prediction.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kaggle competitions submit -c ieee-fraud-detection -f prediction.csv -m \"Message\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, _ = roc_curve(y_valid, valid_preds)\n",
    "plt.plot(fpr, tpr, marker='.', label='nn prediction')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.savefig(\"roc.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "continuous\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
