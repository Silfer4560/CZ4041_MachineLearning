{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Libraries\n",
    "import tqdm\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "# Data processing, metrics and modeling\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold,KFold\n",
    "from bayes_opt import BayesianOptimization\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import precision_score, recall_score, confusion_matrix, accuracy_score, roc_auc_score, f1_score, roc_curve, auc,precision_recall_curve\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "# Lgbm\n",
    "import lightgbm as lgb\n",
    "# Suppr warning\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import itertools\n",
    "from scipy import interp\n",
    "\n",
    "# Plots\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from matplotlib import rcParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file):\n",
    "    return pd.read_csv(file)\n",
    "\n",
    "test = load_data('test_prepared.csv')\n",
    "train = load_data('train_prepared.csv')\n",
    "\n",
    "def nan2mean(df):\n",
    "    for x in list(df.columns.values):\n",
    "        df[x] = df[x].fillna(df[x].mean())\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 355/355 [00:30<00:00, 11.65it/s]\n"
     ]
    }
   ],
   "source": [
    "for col in tqdm.tqdm(train.columns): \n",
    "    if train[col].dtype == 'object':\n",
    "        le = LabelEncoder()\n",
    "        le.fit(list(train[col].astype(str).values) + list(test[col].astype(str).values))\n",
    "        train[col] = le.transform(list(train[col].astype(str).values))\n",
    "        test[col] = le.transform(list(test[col].astype(str).values))  \n",
    "        \n",
    "features = list(train)\n",
    "features.remove('isFraud')\n",
    "target = 'isFraud'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=nan2mean(train)\n",
    "test=nan2mean(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#black box LGBM \n",
    "def LGB_bayesian(\n",
    "    #learning_rate,\n",
    "    num_leaves, \n",
    "    bagging_fraction,\n",
    "    feature_fraction,\n",
    "    min_child_weight, \n",
    "    min_data_in_leaf,\n",
    "    max_depth,\n",
    "    reg_alpha,\n",
    "    reg_lambda\n",
    "     ):\n",
    "    \n",
    "    # LightGBM expects next three parameters need to be integer. \n",
    "    num_leaves = int(num_leaves)\n",
    "    min_data_in_leaf = int(min_data_in_leaf)\n",
    "    max_depth = int(max_depth)\n",
    "\n",
    "    assert type(num_leaves) == int\n",
    "    assert type(min_data_in_leaf) == int\n",
    "    assert type(max_depth) == int\n",
    "    \n",
    "\n",
    "    param = {\n",
    "              'num_leaves': num_leaves, \n",
    "              'min_data_in_leaf': min_data_in_leaf,\n",
    "              'min_child_weight': min_child_weight,\n",
    "              'bagging_fraction' : bagging_fraction,\n",
    "              'feature_fraction' : feature_fraction,\n",
    "              #'learning_rate' : learning_rate,\n",
    "              'max_depth': max_depth,\n",
    "              'reg_alpha': reg_alpha,\n",
    "              'reg_lambda': reg_lambda,\n",
    "              'objective': 'binary',\n",
    "              'save_binary': True,\n",
    "              'seed': 1337,\n",
    "              'feature_fraction_seed': 1337,\n",
    "              'bagging_seed': 1337,\n",
    "              'drop_seed': 1337,\n",
    "              'data_random_seed': 1337,\n",
    "              'boosting_type': 'gbdt',\n",
    "              'verbose': 1,\n",
    "              'is_unbalance': False,\n",
    "              'boost_from_average': True,\n",
    "              'metric':'auc'}    \n",
    "    \n",
    "    oof = np.zeros(len(train))\n",
    "    trn_data= lgb.Dataset(train.iloc[bayesian_tr_idx][features].values, label=train.iloc[bayesian_tr_idx][target].values)\n",
    "    val_data= lgb.Dataset(train.iloc[bayesian_val_idx][features].values, label=train.iloc[bayesian_val_idx][target].values)\n",
    "\n",
    "    clf = lgb.train(param, trn_data,  num_boost_round=50, valid_sets = [trn_data, val_data], verbose_eval=0, early_stopping_rounds = 50)\n",
    "    \n",
    "    oof[bayesian_val_idx]  = clf.predict(train.iloc[bayesian_val_idx][features].values, num_iteration=clf.best_iteration)  \n",
    "    \n",
    "    score = roc_auc_score(train.iloc[bayesian_val_idx][target].values, oof[bayesian_val_idx])\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------------------------------------\n",
      "|   iter    |  target   | baggin... | featur... | max_depth | min_ch... | min_da... | num_le... | reg_alpha | reg_la... |\n",
      "-------------------------------------------------------------------------------------------------------------------------\n",
      "|  1        |  0.9356   |  0.3996   |  0.8606   |  36.33    |  0.005991 |  48.08    |  104.2    |  1.058    |  1.866    |\n",
      "|  2        |  0.9386   |  0.5809   |  0.6665   |  0.04981  |  0.009699 |  169.8    |  130.6    |  1.182    |  1.183    |\n",
      "|  3        |  0.9298   |  0.3434   |  0.5198   |  21.03    |  0.002919 |  130.1    |  96.42    |  1.292    |  1.366    |\n",
      "|  4        |  0.9176   |  0.4649   |  0.7281   |  9.183    |  0.005147 |  126.6    |  52.79    |  1.608    |  1.171    |\n",
      "|  5        |  0.9276   |  0.152    |  0.8591   |  48.25    |  0.008086 |  74.83    |  76.81    |  1.684    |  1.44     |\n",
      "|  6        |  0.9519   |  0.1976   |  0.4961   |  0.7538   |  0.009094 |  66.58    |  341.7    |  1.312    |  1.52     |\n",
      "|  7        |  0.9447   |  0.5374   |  0.2479   |  48.45    |  0.007754 |  189.1    |  450.7    |  1.598    |  1.922    |\n",
      "|  8        |  0.8416   |  0.1708   |  0.2568   |  1.307    |  0.00326  |  89.96    |  158.3    |  1.829    |  1.357    |\n",
      "|  9        |  0.9082   |  0.3247   |  0.5342   |  6.187    |  0.008024 |  33.42    |  493.8    |  1.772    |  1.199    |\n",
      "|  10       |  0.9252   |  0.1044   |  0.7524   |  35.05    |  0.007293 |  158.8    |  65.73    |  1.358    |  1.116    |\n",
      "|  11       |  0.9536   |  0.9      |  0.6473   |  50.0     |  0.01     |  20.0     |  381.9    |  1.0      |  2.0      |\n",
      "=========================================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'bagging_fraction': 0.9,\n",
       " 'feature_fraction': 0.6472974662637391,\n",
       " 'max_depth': 50.0,\n",
       " 'min_child_weight': 0.01,\n",
       " 'min_data_in_leaf': 20.0,\n",
       " 'num_leaves': 381.88914691100524,\n",
       " 'reg_alpha': 1.0,\n",
       " 'reg_lambda': 2.0}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bayesian_tr_idx, bayesian_val_idx = train_test_split(train, test_size = 0.3, random_state = 42, stratify = train[target])\n",
    "bayesian_tr_idx = bayesian_tr_idx.index\n",
    "bayesian_val_idx = bayesian_val_idx.index\n",
    "\n",
    "\n",
    "\n",
    "# Bounded region of parameter space\n",
    "bounds_LGB = {\n",
    "    'num_leaves': (31, 500), \n",
    "    'min_data_in_leaf': (20, 200),\n",
    "    'bagging_fraction' : (0.1, 0.9),\n",
    "    'feature_fraction' : (0.1, 0.9),\n",
    "    #'learning_rate': (0.01, 0.3),\n",
    "    'min_child_weight': (0.00001, 0.01),   \n",
    "    'reg_alpha': (1, 2), \n",
    "    'reg_lambda': (1, 2),\n",
    "    'max_depth':(-1,50),\n",
    "}\n",
    "\n",
    "LGB_BO = BayesianOptimization(LGB_bayesian, bounds_LGB, random_state=42)\n",
    "\n",
    "init_points = 10\n",
    "n_iter = 15\n",
    "\n",
    "print('-' * 130)\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings('ignore')\n",
    "    LGB_BO.maximize(init_points=init_points, n_iter=n_iter, acq='ucb', xi=0.0, alpha=1e-6)\n",
    "\n",
    "\n",
    "LGB_BO.max[\"target\"]\n",
    "\n",
    "\n",
    "\n",
    "LGB_BO.max['params']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
